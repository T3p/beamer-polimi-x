\documentclass{beamer}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}

\usepackage{enumerate}
\usepackage{multimedia}

\usetheme{polimix}
\usepackage{natbib}
 
\title[Safe Policy Optimization]{Safe Policy Optimization}
\subtitle{{\small Ph.D. Course in Information Technology, XXXIII cycle} \\Second Annual Report}

\author[M. Papini]{Matteo Papini}
\supervisor{\small Supervisor}{\small Marcello Restelli}
\date[30/9/2019]{\small 30 Settembre 2019}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\titlepage
\end{frame}

\addtocounter{framenumber}{-1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Motivation}
Apply \textbf{Reinforcement Learning} to \textbf{real-world} control problems
\vfill
\includegraphics[width=\textwidth]{pics/factory.jpg}
\end{frame}

\addtocounter{framenumber}{-1}
\begin{frame}[plain]
\tableofcontents
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Policy Optimization in the Real World}
%\addtocounter{framenumber}{-1}
%\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Reinforcement Learning~\}
\centering
\includegraphics[width=.75\textwidth]{pics/rl2.png}\\
\vspace{.5cm}
{\bf Policy:} agent's behavior ($s\mapsto a$) \\
{\bf Performance $\rho$:} \emph{expected} total reward \\
{\bf Goal:} find policy maximizing performance
\vspace{.25cm}
\begin{itemize}
	\item Model-free
	\item Online
	\item Iterative
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Continuous Reinforcement Learning}
Interesting real-world control problems are \textbf{continuous}
\begin{center}
\includegraphics[width=.5\textwidth]{pics/robots.jpg}
\end{center}
\textbf{Policy Optimization:}
\begin{itemize}
	\item Scales well with state-action dimensionality
	\item Convergence guarantees
	\item Robustness to noise
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Policy Optimization}
\begin{itemize}
	\item Fix a class of controllers with tunable parameters $\boldsymbol{x}\in\mathcal{X}$
	\item Find policy parameters maximizing performance:
	\[	\LARGE
		\max\limits_{\boldsymbol{x}\in\mathcal{X}} \rho(\boldsymbol{x})
	\]
	\vfill
	\item \textbf{Policy Gradient:} solve it with \emph{Stochastic Gradient Descent}
	\vfill
	\begin{columns}
		\begin{column}{.5\textwidth}
			\centering
			\includegraphics[width=.8\textwidth]{pics/dota.jpg}
			\\
			\emph{\cite{OpenAI_dota}}
		\end{column}
		\begin{column}{.5\textwidth}
			\centering
			\includegraphics[width=.8\textwidth]{pics/parkour.jpg}
			\\ 
			\emph{\cite{heess2017emergence}}
		\end{column}
	\end{columns}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Real-World Requirements}
\begin{columns}
	\begin{column}[t]{.5\textwidth}
		{\bf Boss:}\\
		\phantom{"I will improve your controller with RL!"}
		\vspace{1cm}
		\begin{itemize}
			\setlength{\itemsep}{10pt}
			\item How long will it take?
			\item Will it \emph{actually} improve?
			\item Will it behave safely?
			\item How much better will it become?
		\end{itemize}
	\end{column}
	\begin{column}[t]{.5\textwidth}
	{\bf ML Engineer:}\\ 
	"I will improve your controller with RL!"
	\vspace{1cm}
	\begin{itemize}
		\setlength{\itemsep}{10pt}
		\item[] Unknown \hfill \emph{sample complexity}
		\item[] Eventually \hfill \emph{safety}
		\item[] Eventually \hfill \emph{safety}
		\item[] Unknown \hfill \emph{quality of solutions}
	\end{itemize}
	\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Sample Complexity (First Year)}
\textbf{Matteo Papini}, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, Marcello Restelli:
\emph{Stochastic Variance-Reduced Policy Gradient}. \textbf{ICML 2018}: 4023-4032
\vfill
Alberto Maria Metelli, \textbf{Matteo Papini}, Francesco Faccio, Marcello Restelli:
\emph{Policy Optimization via Importance Sampling}. \textbf{NeurIPS 2018}: 5447-5459
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Safe Policy Updates}
\addtocounter{framenumber}{-1}
\frame{\tableofcontents[currentsection]}
\begin{frame}
\frametitle{Safety in Reinforcement Learning}
\begin{itemize}
	\setlength{\itemsep}{10pt}
	\item Many notions of safety ~\citep{amodei2016concrete,garcia2015comprehensive}
	\item Assume performance $\rho$ already encodes \emph{safety constraints}
	\item The optimal policy will be safe
	\item \textbf{The learning process itself may not be!}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Monotonic Performance Improvement}
\begin{itemize}
	\item A concrete problem in Reinforcement Learning
\end{itemize}
\vfill
\centering
\includegraphics[width=.75\textwidth]{example-image-a}
\end{frame}

\begin{frame}
\frametitle{Smoothing Policies and Safe Policy Gradients}
A policy gradient algorithm with \textbf{monotonic improvement} guarantees
\vfill
State of the art~\citep{kakade2002approximately,pirotta2015policy,pirotta2013adaptive,schulman2015trust,papini2017adaptive}:
\begin{itemize}
	\item Restricted policy class
	\item Regularity assumptions on the environment
\end{itemize}
\vfill
Our method:
\begin{itemize}
	\item General conditions on policy
	\item No assumptions on the environment
	\item Simpler formulation
	\item Smaller gap between theory and practice
\end{itemize}
\vfill
Submitted to JMLR
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Safe Exploration}
\addtocounter{framenumber}{-1}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Safe Exploration}
\textbf{Exploration:} perform diverse actions to gather novel information
\begin{itemize}
	\item Necessary for improvement
	\item Tipically: perform \emph{random} actions
	\item Unpredictable behavior may be \textbf{unsafe}
\end{itemize}
\vfill
Control the amount of stochasticity:
\vspace{.3cm}

\textbf{Matteo Papini}, Andrea Battistello, Marcello Restelli; \emph{“Safely Exploring Policy Gradient”}; 14th European Workshop on Reinforcement Learning (\textbf{EWRL14}), Lille, France, 2018

\vfill
Revised version planned for \textbf{AISTATS 2020}
\end{frame}

%\begin{frame}
%\frametitle{Safely Exploring Policy Gradients (?)}
%\end{frame}

\begin{frame}
\frametitle{OPTIMIST}
\textbf{Direct} exploration towards interesting information

\vfill
\emph{Not} policy gradient, inspired by Multi-Armed-Bandit literature~\citep{bubeck2012regret,lattimore2018bandit}

\vfill
\begin{itemize}
	\item Global convergence
	\item Still requires stochasticity
	\item Non-monotonic performance
\end{itemize}
\vfill
Published at \textbf{ICML 2019}

\end{frame}

\begin{frame}
\frametitle{(Truly) Deterministic Policy Gradient}
\begin{itemize}
	\setlength{\itemsep}{10pt}
	\item Is (random) exploration really necessary?
	\item Maybe not if the world is sufficiently regular
	\item \textbf{Idea}: re-use similar experience instead of sampling new one
\end{itemize}

\vfill
Planned for \textbf{ICML 2020}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Quality of Solutions}
\addtocounter{framenumber}{-1}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Non-Convexity}
\begin{itemize}
	\setlength{\itemsep}{10pt}
	\item The performance objective $\rho$ is \textbf{nonconvex}
	\item Policy gradient only converges to \textbf{local optima}
	\item Locally optimal performance could be poor
	\item Locally optimal policies may be \textbf{unsafe}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Global Optimality Guarantees}
\begin{itemize}
	\setlength{\itemsep}{10pt}
	\item The policy optimization problem is \emph{special}
	\item Convergence to the \textbf{global optimum} is possible in some cases~\citep{bhandari2019global,agarwal2019optimality,shani2019adaptive}
	\item Can we exploit tools from \textbf{convex optimization} to design \emph{new} algorithms?
\end{itemize}
\vfill
Possible target: \textbf{NeurIPS 2020}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Wrapping Up}
Policy optimization in the real world:
\begin{itemize}
	\setlength{\itemsep}{10pt}
	\item How long will it take?~\citep{papini2018stochastic,xu2019improved,xu2019sample}
	\item Will it \emph{actually} improve?~\citep{papini2019smoothing}
	\item Will it behave safely? (work in progress)
	\item How much better will it become? (future work)
\end{itemize}
\vspace{1cm}
We also want to:
\begin{columns}
	\begin{column}{.5\textwidth}
		\begin{itemize}
			\item Find a trade-off between competing goals
			\item Apply to a real problem
		\end{itemize}
	\end{column}
	\begin{column}{.5\textwidth}
		\includegraphics[width=\textwidth]{pics/pirelli.jpg}
	\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Publications}
\emph{First year:}
\begin{itemize}
	\small
	\setlength{\itemsep}{10pt}
	\item  \textbf{Matteo Papini}, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, Marcello Restelli:
	\emph{Stochastic Variance-Reduced Policy Gradient}. \textbf{ICML 2018}: 4023-4032
	\item  Alberto Maria Metelli, \textbf{Matteo Papini}, Francesco Faccio, Marcello Restelli:
	\emph{Policy Optimization via Importance Sampling}. \textbf{NeurIPS 2018}: 5447-5459
\end{itemize}

\vfill
\emph{Previous:}
\begin{itemize}
	\small
	\setlength{\itemsep}{10pt}
	\item  \textbf{Matteo Papini}, Matteo Pirotta, Marcello Restelli:
	\emph{Adaptive Batch Size for Safe Policy Gradients}. \textbf{NeurIPS 2017}: 3591-3600
\end{itemize}

\vfill
\emph{Workshop papers:}
\begin{itemize}
	\small
	\item  \textbf{Matteo Papini}, Andrea Battistello, Marcello Restelli: \emph{Safely Exploring Policy Gradient}. EWRL 2018 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Publications}
\emph{Second year:}
\begin{itemize}
	\small
	\setlength{\itemsep}{10pt}
	\item \textbf{Matteo Papini}, Alberto Maria Metelli, Lorenzo Lupo, Marcello Restelli:
	\emph{Optimistic Policy Optimization via Multiple Importance Sampling}. \textbf{ICML 2019}: 4989-4999
	\item Mario Beraha, Alberto Maria Metelli, \textbf{Matteo Papini}, Andrea Tirinzoni, Marcello Restelli:
	\emph{Feature Selection via Mutual Information: New Theoretical Insights}. IJCNN 2019
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Submissions}
\begin{itemize}
	\small
	\setlength{\itemsep}{10pt}
	\item  \textbf{Matteo Papini}, Matteo Pirotta, Marcello Restelli:
	\emph{Smoothing Policies and Safe Policy Gradients}. (\textbf{JMLR})
	\item  Pierluca D'Oro, Alberto Maria Metelli, Andrea Tirinzoni, \textbf{Matteo Papini}, Marcello Restelli:
	\emph{Gradient-Aware Model-based Policy Search}. (\textbf{AAAI 2020})
	\item   Lorenzo Bisi, Luca Sabbioni, Edoardo Vittori, \textbf{Matteo Papini}, Marcello Restelli: \emph{Risk-Averse Trust Region Optimization for Reward-Volatility Reduction} (\textbf{AAAI 2020})
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Courses and Schools}
\textbf{Courses (25/25 CFU):}

\includegraphics[width=\textwidth]{pics/corsi.png}
\vfill

\textbf{Schools:}
\begin{itemize}
	\item  Deep Learning and Reinforcement Learning Summer School (DLRLSS), Toronto, Canada, 2018
	\item  ACAI Summer School on Reinforcement Learning, Nieuwpoort, Belgium, 2017
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Teaching}
\textbf{2017/2018}
\begin{itemize}
	\setlength{\itemsep}{10pt}
	\item  Responsabile di laboratorio, Informatica B, Prof. Luca Cassano
	\item  Esercitatore, Web and Internet Economics, Prof. Nicola Gatti
\end{itemize}

\vfill
\textbf{2018/2019}
\begin{itemize}
	\setlength{\itemsep}{10pt}
	\item  Esercitatore, Informatica B, Prof. Luca Cassano
\end{itemize}

\vfill
\textbf{2019/2020}
\begin{itemize}
	\setlength{\itemsep}{10pt}
	\item  Esercitatore, Informatica B, Prof. Luca Cassano (now)
\end{itemize}

\vfill
\textbf{Teaching outside Politecnico:}
\begin{itemize}
	\setlength{\itemsep}{10pt}
	\item Teaching Assistant for the Reinforcement Learning Summer School (RLSS), Lille, France, 2019
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Other Activities}
\scriptsize
\textbf{Talks and posters:}
\begin{itemize}
	\item Seminar \emph{Temporal Credit Assignment in Off-Policy Reinforcement Learning}, DEIB, November 28th , 2017
	\item Poster presentation at NeurIPS 2017
	\item Oral and poster presentation at ICML 2018
	\item Poster presentation at DLRLSS 2018
	\item Poster presentation at EWRL14 (2018)
	\item Oral and poster presentation at NeurIPS 2018
	\item Oral and poster presentation at ICML 2019
	\item Invited talk at MAPLE workshop 2019
\end{itemize}

\vfill
\textbf{Editorial activities:}
\begin{itemize}
	\item Subreviewer for IJCAI 2018
	\item Reviewer for ICML 2019
	\item PC Member for UAI 2019
	\item Reviewer for NeurIPS 2019
	\item Reviewer for AAAI 2020 (now)
	\item Reviewer for AISTATS 2020 (planned)
\end{itemize}

\vfill
\textbf{Co-supervised master students:} G. Canonaco, D. Binaghi, A. Battistello, F. Faccio, A. Mongelluzzo, L. Lupo, G. Pelosi, P. Melzi (now)
\end{frame}



\begin{frame}[plain]
\centering
{\color{poliblue3} \bf
\vspace{1cm}
{\huge Thank you for your attention!} \\
\vspace{2cm}
{\LARGE Questions?}
}
\end{frame}


\begin{frame}[allowframebreaks]
\frametitle{Bibliography}
\scriptsize
\bibliographystyle{plainnat}
\bibliography{slides}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document} 
